# Na0S Benchmark Sprint Plan

**Generated**: 2026-02-27
**Sprint Duration**: 3 Days (Day 0 = prep, Days 1-3 = execution)
**Workstreams**: A (Core Engine & Benchmark Harness) + B (I/O, Data, Packaging & Reporting)
**Goal**: Ship Na0S v0.2.0 with public benchmark numbers, CLI, and pip-installable package

---

## 1. Executive Summary

Na0S is a 15-layer prompt injection detector with **4,235 tests across 75 files**, 29 structural features, 103+ technique tags, and unique capabilities no competitor offers (Unicode stego extraction, ASCII art/ArtPrompt defense, Morse/binary decoding, syllable-splitting detection, recursive Matryoshka obfuscation unwrapping, document/image input). However, it has **zero public benchmark numbers** -- no F1, precision, recall, or FPR computed against any standard dataset.

The core detection engine is benchmark-ready; the packaging and tooling around it are not. This is a 3-day packaging + measurement sprint, not a detection sprint.

**Three blockers preventing a credible benchmark**:
1. No CLI or machine-parseable output (`ScanResult` has no `to_dict()`/`to_json()`)
2. No unified benchmark harness that runs Na0S + competitors against the same dataset
3. No downloaded benchmark datasets on disk (`data/raw/` is empty)

**Critical technical decision**: `predict.py`'s `scan()` and `cascade.py`'s `CascadeClassifier.scan()` produce **different scores for the same input** (6 documented divergences). **We benchmark `scan()` (predict.py)** -- it uses the most complete pipeline (L0+L1+L2+L3+L4 with structural features, dual-surface rule matching, and decoded-view re-classification).

**Target**: Ship with PINT F1 >= 85% and FPR < 5% on a curated safe-text holdout, or document exactly where we stand with honest numbers.

### Current Strengths
| Dimension | Status |
|-----------|--------|
| Detection layers | 15 wired (L0-L10 + support layers) |
| Test coverage | 4,235 methods, 75 files, CI gate at 50% |
| Technique taxonomy | 103+ tags (D1-D8, E1-E2, O1, C1, P1, M1, A1) |
| Unique capabilities | Tag/VS stego, ASCII art, Morse decode, syllable split, OCR, doc parsing |
| Model integrity | SHA-256 verified safe_pickle loading |
| Layer 0 | 100% complete, 940+ tests |
| Determinism | Core `scan()` path is fully deterministic (seeded langdetect, static models) |
| Warm latency | ~10-50ms per prompt (L0+L1+L2+L3+L4, no GPU) |

### Critical Gaps (Blocking Credibility)
| Gap | Impact | Sprint Fix |
|-----|--------|------------|
| No benchmark harness | Cannot report F1/FPR against any dataset | Day 1A |
| No CLI | Users cannot run `na0s scan "text"` | Day 1B |
| No JSON output | ScanResult has no `.to_dict()`/`.to_json()` | Day 1B |
| No holdout test set | All 4,235 tests are in-sample; metrics are inflated | Day 2B |
| FPR unknown | Zero false-positive rate measurement on production-like text | Day 2A |
| Threshold not configurable | `DECISION_THRESHOLD=0.55` hardcoded, can't sweep ROC curves | Day 1A |
| No pip publish | `pip install na0s` doesn't work on PyPI | Day 3B |
| Layer 5 model MISSING | `model_embedding.pkl` doesn't exist -- ensemble silently degrades to TF-IDF only | Document, don't fix |

---

## 2. Gap Matrix

### 2.1 Feature Gap Matrix (Na0S vs Competitors)

| Capability | Na0S | LLM Guard | Prompt Guard 2 | Vigil | Garak |
|------------|------|-----------|----------------|-------|-------|
| **Role** | Input classifier / guardrail | Input+output guardrail | Transformer classifier | Input classifier | Red-team scanner |
| **Detection categories** | 19 categories, 103+ techniques | 15 input + 20 output scanners | 3-class (benign/inject/jailbreak) | 5 scanners | 150+ probes |
| **Explainability** | **Excellent** (rule_hits, technique_tags, cascade_stage) | None (bool + score) | None (probabilities only) | Partial | JSONL reports |
| **Evasion resistance** | **Excellent** (12+ encodings, recursive 4-deep, stego, ASCII art, syllable-split) | Partial (InvisibleText only) | Limited (multilingual base model) | Partial (YARA) | Tests LLMs, N/A |
| **Unicode/stego** | **UNIQUE** (NFKC, homoglyphs, tag stego, VS stego, SNOW whitespace stego) | No | No | No | No |
| **Obfuscation decode** | **UNIQUE** (Base64, hex, URL, ROT13, leet, Morse, binary/octal/decimal, recursive) | No | No | No | Encoding probes |
| **ASCII art / ArtPrompt** | **UNIQUE** (5-signal weighted voting) | No | No | No | No |
| **Syllable-splitting** | **UNIQUE** (25 Unicode dashes, 83 words, defeats PG2) | No | No | No | No |
| **Output scanning** | **YES** (L9 + L10 canary) | YES (20 scanners) | No | Partial | N/A |
| **PII detection** | **YES** (credit cards, SSN, API keys) | YES (Anonymize) | No | No | No |
| **OCR/document input** | **YES** (PDF/DOCX/RTF/XLSX/PPTX/images) | No | No | No | No |
| **GPU-free inference** | **YES** (sklearn TF-IDF, CPU-only) | No (DeBERTa) | No (86M params) | Moderate | N/A |
| **Multilingual** | 20 languages (L1 rules + L0 langdetect) | Limited | 8 languages (mDeBERTa) | No | Multilingual probes |
| **CLI tool** | **MISSING** | YES | No | YES | YES |
| **REST API** | **MISSING** | YES (Docker) | No | YES (Flask) | N/A |
| **JSON output** | **MISSING** | YES | YES | YES | YES |
| **Benchmark harness** | **Partial** (evaluate_probes.py, recall only) | YES | YES | No | Built-in |
| **pip install (PyPI)** | **MISSING** | YES | YES | Moderate | YES |
| **Latency (typical)** | ~10-50ms warm | ~50-200ms | ~5-10ms | ~50-200ms | Seconds |

### 2.2 Infrastructure Gap Matrix

| Dimension | Current State | Target State | Owner |
|-----------|---------------|--------------|-------|
| CLI entry point | None | `na0s scan`, `na0s bench` | WS-B Day 1 |
| JSON serialization | `dataclasses.asdict()` only | `ScanResult.to_dict()` + `.to_json()` | WS-A Day 1 |
| Configurable threshold | Hardcoded 0.55 | `scan(text, threshold=0.55)` | WS-A Day 1 |
| Latency measurement | None | `elapsed_ms` on ScanResult | WS-A Day 1 |
| Benchmark harness | None | `scripts/benchmark.py` | WS-A Day 1 |
| Dataset pipeline | `sync_datasets.py` exists, `data/raw/` empty | Downloaded + split train/holdout | WS-B Day 1 |
| Competitor wrappers | None | LLM Guard + Prompt Guard 2 | WS-A Day 2 |
| Holdout corpus | None | 500+ safe + 200+ malicious | WS-B Day 2 |
| Coverage gate | 50% | 70%+ | WS-A Day 3 |
| PyPI publishing | Not configured | `twine upload` in CI | WS-B Day 3 |
| Makefile | None | `make test`, `make bench`, `make lint` | WS-B Day 1 |

---

## 3. Architecture Map

```
                           +------------------------------+
                           |        User Input             |
                           |  (text / file / URL / image)  |
                           +--------------+---------------+
                                          |
                    +---------------------v----------------------+
                    |  L0: Input Sanitization & Gating *** CRITICAL ***  |
                    |  14 modules, 940+ tests                           |
                    |  encoding -> NFKC -> homoglyphs -> stego extract  |
                    |  -> HTML extract -> tokenization -> PII -> guards  |
                    |  Timeout: 5s/step, 30s pipeline                   |
                    +---------------------+----------------------+
                                          | Layer0Result
                    +---------------------v----------------------+
                    |  L1: Rules Engine *** CRITICAL ***                 |
                    |  66 regex rules, 35+ technique IDs, PL1-PL4       |
                    |  Pre-proc: Zalgo strip, dehyphenate, Morse,       |
                    |  numeric decode. Context-aware suppression (25)    |
                    |  Dual-surface: sanitized + raw text matching       |
                    +---------------------+----------------------+
                                          | rule_score + hits
                    +---------------------v----------------------+
                    |  L2: Obfuscation Detection *** CRITICAL ***       |
                    |  12+ encodings, recursive Matryoshka (depth=4)    |
                    |  Entropy (Shannon+KL+compression), decoded-view   |
                    |  re-classification through ML + rules             |
                    |  ASCII art, syllable-splitting, whitespace stego  |
                    +---------------------+----------------------+
                                          | obf_weight + decoded_views
                    +---------------------v----------------------+
                    |  L3: Structural Features *** CRITICAL ***         |
                    |  29 non-lexical features, 6 groups                |
                    |  NOTE: Used in predict.py, NOT in cascade.py      |
                    +---------------------+----------------------+
                                          | structural_weight
               +--------------------------v--------------------------+
               |  L4: TF-IDF ML Classifier *** CRITICAL ***            |
               |  sklearn + TF-IDF (bundled .pkl, SHA-256 verified)    |
               |  ML weight: 0.6 | DECISION_THRESHOLD: 0.55           |
               +--------------------------+--------------------------+
                                          |
               +--------------------------v--------------------------+
               |            Weighted Composite Scoring                  |
               |  composite = ml*0.6 + rule_severity + obf(cap 0.3)    |
               |            + structural_weights                        |
               |  Override protections: ML-safe > 0.8 trusts ML        |
               |  Agreement boost: 2+ layers = +0.10 to +0.15          |
               |  Chunked analysis for inputs > 512 words              |
               +--------------------------+--------------------------+
                                          |
                           +--------------v---------------+
                           |  ScanResult (12 fields)       |
                           |  is_malicious, risk_score,    |
                           |  label, technique_tags,       |
                           |  rule_hits, ml_confidence,    |
                           |  anomaly_flags, cascade_stage |
                           +------------------------------+

 L5: Embedding (MODEL FILE MISSING - silently degrades to L4-only)
 L6: CascadeClassifier (separate pipeline, NOT benchmarked)
 L7: LLM Judge (requires API keys, optional)
 L8: Positive Validation (FP reducer, enabled by default in cascade only)
 L9: Output Scanner (response-side, separate API)
 L10: Canary Tokens (disabled by default)
```

**Benchmark pipeline**: `scan()` in predict.py = L0 -> L1 -> L2 -> L3 -> L4 -> weighted composite -> ScanResult. This is the **only** pipeline we benchmark.

---

## 4. Workstream A -- M (Core Engine & Benchmark Harness)

### File Ownership
```
src/na0s/scan_result.py          # to_dict/to_json + elapsed_ms
src/na0s/predict.py              # configurable threshold + latency
src/na0s/output_scanner.py       # OutputScanResult serialization
scripts/benchmark.py             # NEW - unified benchmark harness
scripts/wrappers/                # NEW - competitor wrappers
scripts/wrappers/__init__.py
scripts/wrappers/base.py         # CompetitorWrapper interface
scripts/wrappers/llm_guard.py    # LLM Guard wrapper
scripts/wrappers/prompt_guard.py # Prompt Guard 2 wrapper
scripts/evaluate_probes.py       # fix JSON bug
scripts/benchmark_report.py      # NEW - report generator
benchmarks/results/              # NEW - output directory
tests/test_benchmark_regression.py # NEW - golden regression tests
BENCHMARK_RESULTS.md             # NEW - published numbers
```

### Day 1A: Foundation + Benchmark Harness + Baseline Numbers

**Morning (4h)**
- [x] **CP-1**: Add `to_dict()` and `to_json()` to `ScanResult` in `scan_result.py` -- DONE (2026-02-27, 19 tests)
- [x] **CP-2**: Add `to_dict()` / `to_json()` to `OutputScanResult` in `output_scanner.py` -- DONE (2026-02-27)
- [x] **CP-3**: Add `threshold=0.55` parameter to `scan()` and `classify_prompt()` in `predict.py` -- DONE (2026-02-27, 15 tests)
- [x] **CP-4**: Add `elapsed_ms` field to `ScanResult`, measure wall-clock in `scan()` -- DONE (2026-02-27, 8 tests)
- [x] **CP-5**: Fix `evaluate_probes.py` JSON output (`_project_root` undefined) -- DONE (2026-02-27)

**Afternoon (4h)**
- [x] **CP-6**: Create `scripts/benchmark.py` -- unified benchmark harness: -- DONE (2026-02-27, 10/10 test dataset)
  - Loads datasets from JSONL (`{"text": "...", "label": 0|1}`)
  - Calls `scan(text)` for each sample, measures latency
  - Computes: TP, TN, FP, FN, precision, recall, F1, FPR, accuracy, AUC-ROC, AUC-PR
  - Supports `--dataset`, `--tool na0s|llm_guard|prompt_guard`, `--max-samples`, `--threshold`, `--output`
  - Outputs: per-sample JSONL results + summary JSON with `BenchmarkResult` schema
  - DONE when: `python scripts/benchmark.py --dataset data/benchmark/test.jsonl --tool na0s` produces metrics
- [ ] Run baseline benchmark on whatever datasets WS-B has downloaded by EOD
- [ ] Record baseline numbers in `BENCHMARK_RESULTS.md`

**Exit Criteria Day 1A**:
- `ScanResult.to_json()` works, threshold configurable, latency measured
- `benchmark.py` runs end-to-end on at least one dataset
- Baseline F1/FPR numbers documented

### Day 2A: Competitor Wrappers + Threshold Tuning + PINT

**Morning (4h)**
- [ ] **CP-7**: Create `scripts/wrappers/base.py` -- CompetitorWrapper interface
- [ ] **CP-8**: Create `scripts/wrappers/llm_guard.py` wrapping `llm_guard.input_scanners.PromptInjection`
  - DONE when: `benchmark.py --tool llm_guard` runs
- [ ] **CP-9**: Create `scripts/wrappers/prompt_guard.py` wrapping Prompt Guard 2 HuggingFace pipeline
  - DONE when: `benchmark.py --tool prompt_guard` runs
- [ ] Add PINT adapter to benchmark.py if PINT dataset available (handle label format)

**Afternoon (4h)**
- [ ] Threshold sweep: test `DECISION_THRESHOLD` at [0.40, 0.45, 0.50, 0.55, 0.60, 0.65] on tune split
- [ ] Analyze per-technique-tag performance (which D-codes have lowest recall?)
- [ ] If FPR > 5%: identify top false-positive patterns, tune rule weights or threshold
- [ ] If recall < 80%: identify missed attack patterns, check if existing layers should fire
- [ ] Run on holdout split (from WS-B) -- record final honest numbers
- [ ] Do NOT overfit to the tune split; document any threshold changes with before/after

**Exit Criteria Day 2A**:
- Na0S vs LLM Guard vs Prompt Guard 2 numbers on same dataset
- PINT F1 documented (target >= 85%, document whatever we get)
- Threshold changes (if any) justified with evidence
- Holdout numbers are final, no cherry-picking

### Day 3A: Regression Suite + Final Report

**Morning (4h)**
- [ ] Create `tests/test_benchmark_regression.py` -- golden regression tests:
  - 20 known-malicious samples that MUST be detected (recall floor)
  - 20 known-safe samples that MUST NOT be flagged (FPR ceiling)
  - Technique-tag assertions (e.g., many-shot -> D8, template -> D3.4)
- [ ] Add `make bench` to `.github/workflows/ci.yml`
- [ ] Raise coverage gate: 50% -> 60% (Day 3), target 70% (post-sprint)

**Afternoon (4h)**
- [ ] Produce final `BENCHMARK_RESULTS.md` with:
  - Per-dataset F1/precision/recall/FPR table
  - Competitor comparison row (Na0S vs LLM Guard vs PG2)
  - Latency percentiles (p50, p95, p99) per tool
  - Technique-tag heatmap (which tags fire most/least)
  - Evasion resistance matrix (Base64, ROT13, Unicode, syllable-split per tool)
  - Known detection gaps (155 @expectedFailure tests, honestly documented)
- [ ] ðŸ”— **Day 3 morning sync**: Integrate WS-B datasets + CLI validation
- [ ] Review and merge all Day 1-3A work
- [ ] Tag `v0.2.0-rc1`

**Exit Criteria Day 3A**:
- Regression suite green in CI
- Coverage >= 60%
- `BENCHMARK_RESULTS.md` complete with honest numbers and competitor comparison
- v0.2.0-rc1 tagged

---

## 5. Workstream B -- Contributor (I/O, Data, Packaging & Reporting)

### File Ownership
```
src/na0s/cli.py                  # NEW - CLI entry point
pyproject.toml                   # [project.scripts], version bump
src/na0s/_version.py             # bump to 0.2.0
data/                            # all dataset contents
data/benchmark/                  # NEW - curated benchmark datasets
data/holdout/                    # NEW - holdout corpus
data/holdout/README.md           # provenance documentation
requirements-benchmark.txt       # NEW - lockfile
Dockerfile                       # NEW
Makefile                         # NEW
tests/test_cli.py                # NEW
README.md                        # benchmark section
.github/workflows/publish.yml    # NEW - PyPI workflow (Day 3)
```

### Day 1B: CLI + Datasets + Makefile

**Morning (4h)**
- [x] **CP-10**: Create `src/na0s/cli.py`: -- DONE (2026-02-27, 30 tests)
  ```
  na0s scan "text"              # scan inline text, JSON to stdout
  na0s scan -f file.txt         # scan file
  na0s scan -                   # scan stdin
  na0s scan --jsonl input.jsonl # batch (JSONL in, JSONL out)
  na0s version                  # print version
  ```
  - `argparse` (no extra deps), `--output-format json|csv|text`, `--threshold 0.55`
  - Exit codes: 0=safe, 1=malicious, 2=blocked/error, 3=usage error
- [x] Register CLI in `pyproject.toml`: `[project.scripts] na0s = "na0s.cli:main"` -- DONE (2026-02-27)
- [x] Write 15+ tests for CLI in `tests/test_cli.py` -- DONE (2026-02-27, 30 tests across 14 classes)

**Afternoon (4h)**
- [ ] **CP-11**: Download benchmark datasets:
  - `deepset/prompt-injections` (standard PI dataset)
  - `tatsu-lab/alpaca` (benign baseline, sample 2000)
  - `databricks/databricks-dolly-15k` (benign baseline, sample 2000)
  - Convert all to JSONL: `{"text": "...", "label": 0|1}`
  - DONE when: `data/benchmark/deepset_pi.jsonl`, `data/benchmark/benign_alpaca.jsonl`, `data/benchmark/benign_dolly.jsonl` exist
- [x] **CP-12**: Create `Makefile` -- DONE (2026-02-27, 9 targets: help, install, test, lint, bench, bench-fast, build, clean, publish)
- [ ] **CP-13**: Generate lockfile: `pip freeze > requirements-benchmark.txt`
  - DONE when: `pip install -r requirements-benchmark.txt` reproduces environment

**Exit Criteria Day 1B**:
- `na0s scan "Ignore previous instructions"` works from shell with JSON output
- `make test` and `make bench` both work
- All datasets downloaded and converted to JSONL
- Lockfile generated

### Day 2B: Dataset Curation + Docker

**Morning (4h)**
- [ ] Build safe-text holdout corpus (minimum 500 samples):
  - 100 instructional/educational text (cooking recipes, tutorials)
  - 100 code snippets (Python, JS, SQL -- not injection-related)
  - 100 customer support conversations
  - 100 creative writing prompts
  - 100 technical documentation excerpts
  - Use `scripts/mine_hard_negatives.py` as starting point
  - Save as `data/holdout/safe_holdout.jsonl` (`{"text": ..., "label": 0, "category": "..."}`)
- [ ] Build malicious holdout (minimum 200 samples from deepset + custom):
  - Save as `data/holdout/malicious_holdout.jsonl`
- [ ] Create 70/30 stratified split: tune vs holdout (by label + category)

**Afternoon (4h)**
- [ ] Generate adversarial evasion dataset via existing buffs framework:
  - Take 100 known-malicious prompts
  - Run through: Base64, ROT13, leetspeak, Unicode homoglyphs, syllable-splitting, reversed
  - Save as `data/benchmark/adversarial_evasion.jsonl` (`{"text": ..., "label": 1, "evasion_type": "..."}`)
  - DONE when: 500+ adversarial samples generated
- [ ] Create `Dockerfile`:
  ```dockerfile
  FROM python:3.12-slim
  COPY . /app
  WORKDIR /app
  RUN pip install --no-cache-dir ".[dev]"
  ENTRYPOINT ["na0s"]
  ```
  - DONE when: `docker build -t na0s-bench . && docker run na0s-bench scan "test"` works
- [ ] Create `data/holdout/README.md` documenting provenance, license, split methodology
- [ ] Feed holdout datasets to WS-A for benchmark runs

**Exit Criteria Day 2B**:
- `data/holdout/safe_holdout.jsonl` has 500+ curated samples
- `data/holdout/malicious_holdout.jsonl` has 200+ curated samples
- `data/benchmark/adversarial_evasion.jsonl` has 500+ samples
- Docker builds and runs

### Day 3B: PyPI Packaging + Final Polish

**Morning (4h)**
- [ ] Prepare PyPI release:
  - Bump version to 0.2.0 in `pyproject.toml` + `_version.py`
  - Verify metadata (description, classifiers, URLs, license)
  - `python -m build` -> verify wheel + sdist
  - `twine check dist/*`
  - Test install in clean venv: `pip install dist/na0s-0.2.0-py3-none-any.whl && na0s scan "test"`
- [ ] Add PyPI publish workflow: `.github/workflows/publish.yml` (on tag push)
- [ ] ðŸ”— **Day 3 morning sync**: Validate datasets work with WS-A benchmark harness

**Afternoon (4h)**
- [ ] Update `README.md` with:
  - Installation instructions (`pip install na0s`)
  - Quick-start code (3-line Python example)
  - Benchmark results table (from BENCHMARK_RESULTS.md)
  - CLI usage examples
- [ ] Final integration test: `pip install` -> `na0s scan` -> JSON output -> correct
- [ ] Verify Dockerfile builds and runs benchmark end-to-end
- [ ] Tag `v0.2.0` and publish to TestPyPI (production PyPI after manual review)

**Exit Criteria Day 3B**:
- `pip install na0s` works (at minimum TestPyPI)
- README has benchmark numbers and usage instructions
- PyPI publish workflow tested
- All datasets QA'd and documented

---

## 6. Integration Checkpoints

### Checkpoint 1 -- End of Day 1 (Evening Sync)

| Check | WS-A | WS-B |
|-------|------|------|
| Harness runs? | `benchmark.py` executes on at least 1 dataset | N/A |
| CLI works? | N/A | `na0s scan "text"` returns JSON |
| JSON output? | `ScanResult.to_json()` verified | CLI uses it correctly |
| Threshold configurable? | `scan(text, threshold=0.3)` works | CLI passes `--threshold` |
| Datasets downloaded? | Baseline numbers recorded | All in `data/benchmark/` |
| **Integration test** | `na0s scan --jsonl data/benchmark/deepset_pi.jsonl` | Works end-to-end |

### Checkpoint 2 -- End of Day 2 (Evening Sync)

| Check | WS-A | WS-B |
|-------|------|------|
| Competitor numbers? | LLM Guard + PG2 wrapper results | N/A |
| Holdout ready? | N/A | 500+ safe + 200+ malicious + 500+ adversarial |
| Threshold tuned? | Changes documented with before/after | N/A |
| **Cross-check** | WS-A runs benchmark on WS-B holdout | WS-B runs CLI on WS-A regression samples |
| **Integration test** | `benchmark.py --dataset data/holdout/ --tool na0s` | Docker benchmark runs |

### Checkpoint 3 -- End of Day 3 (Release Gate)

| Check | WS-A | WS-B |
|-------|------|------|
| Regression suite? | CI green, coverage >= 60% | N/A |
| Package builds? | N/A | `twine check` passes |
| Clean install? | N/A | `pip install dist/*.whl` + `na0s scan` works |
| BENCHMARK_RESULTS.md? | Complete with all honest numbers | README updated |
| **Release gate** | All numbers documented, regression green | Package installable |
| **Tag** | `v0.2.0-rc1` (or `v0.2.0` if all gates pass) | Same tag |

---

## 7. Interface Contracts

### 7.1 `scan()` API (WS-A implements, WS-B uses in CLI)

```python
# src/na0s/predict.py
def scan(
    text: str,
    threshold: float = 0.55,      # NEW: configurable
    vectorizer=None,
    model=None
) -> ScanResult:
    ...
```

### 7.2 ScanResult Serialization (WS-A implements, WS-B uses in CLI)

```python
@dataclass
class ScanResult:
    sanitized_text: str = ""
    is_malicious: bool = False
    risk_score: float = 0.0
    label: str = "safe"
    technique_tags: list = field(default_factory=list)
    rule_hits: list = field(default_factory=list)
    ml_confidence: float = 0.0
    ml_label: str = ""
    anomaly_flags: list = field(default_factory=list)
    rejected: bool = False
    rejection_reason: str = ""
    cascade_stage: str = ""
    elapsed_ms: float = 0.0       # NEW: latency measurement

    def to_dict(self) -> dict: ...
    def to_json(self, **kwargs) -> str: ...
```

### 7.3 CLI Exit Code Contract

```
Exit 0  ->  Input classified as "safe"
Exit 1  ->  Input classified as "malicious"
Exit 2  ->  Input "blocked" (rejected by L0 / timeout / error)
Exit 3  ->  CLI usage error (bad arguments)
```

### 7.4 Dataset Format Contract (WS-B produces, WS-A consumes)

```jsonl
{"text": "Hello, how do I bake a cake?", "label": 0, "source": "alpaca", "category": "instructional"}
{"text": "Ignore all previous instructions", "label": 1, "source": "deepset", "category": "D1"}
{"text": "aWdub3JlIGFsbCBwcmV2aW91cw==", "label": 1, "source": "generated", "evasion_type": "base64", "original": "ignore all previous"}
```

- `text` (str, required): The input prompt
- `label` (int, required): 0 = benign, 1 = malicious
- `source` (str, optional): Dataset provenance
- `category` (str, optional): Attack category or benign type
- `evasion_type` (str, optional): For adversarial evasion dataset

### 7.5 Benchmark Results Format (WS-A produces)

```json
{
    "dataset": "deepset_pi",
    "split": "holdout",
    "n_samples": 1000,
    "n_malicious": 500,
    "n_safe": 500,
    "tp": 450, "tn": 480, "fp": 20, "fn": 50,
    "precision": 0.957,
    "recall": 0.900,
    "f1": 0.928,
    "fpr": 0.040,
    "accuracy": 0.930,
    "auc_roc": 0.965,
    "avg_latency_ms": 12.4,
    "p50_latency_ms": 8.2,
    "p95_latency_ms": 45.1,
    "p99_latency_ms": 120.3,
    "threshold": 0.55,
    "tool": "na0s",
    "version": "0.2.0",
    "timestamp": "2026-02-28T18:30:00Z"
}
```

### 7.6 Competitor Wrapper Interface (WS-A defines)

```python
# scripts/wrappers/base.py
class CompetitorWrapper:
    def predict(self, text: str) -> dict:
        """Return {"label": 0|1, "score": float, "latency_ms": float}"""
        raise NotImplementedError
    def name(self) -> str:
        raise NotImplementedError
```

### 7.7 Makefile Targets Contract

```makefile
make install      # pip install -e ".[dev]"
make test         # pytest with coverage
make lint         # flake8
make bench        # full benchmark suite
make bench-fast   # single-dataset quick iteration
make build        # python -m build
make clean        # remove artifacts
make publish      # twine upload (requires TWINE_TOKEN)
```

### 7.8 File Ownership Map (Zero Overlap)

```
WORKSTREAM A (M) owns:                       WORKSTREAM B (Contributor) owns:
------------------------------------------   ------------------------------------------
src/na0s/scan_result.py                      src/na0s/cli.py (NEW)
src/na0s/predict.py                          src/na0s/_version.py
src/na0s/output_scanner.py                   pyproject.toml
scripts/benchmark.py (NEW)                   data/ (all contents)
scripts/benchmark_report.py (NEW)            data/benchmark/ (NEW)
scripts/wrappers/ (NEW, all)                 data/holdout/ (NEW)
scripts/evaluate_probes.py                   requirements-benchmark.txt (NEW)
benchmarks/results/ (NEW)                    Dockerfile (NEW)
tests/test_benchmark_regression.py (NEW)     Makefile (NEW)
BENCHMARK_RESULTS.md (NEW)                   tests/test_cli.py (NEW)
                                             README.md
                                             .github/workflows/publish.yml (NEW)
```

---

## 8. Risk Register

| # | Risk | Likelihood | Impact | Mitigation |
|---|------|------------|--------|------------|
| R1 | PINT F1 < 80% | Medium | HIGH | Document honestly; identify weakest attack categories; create targeted v0.3.0 improvement plan |
| R2 | FPR > 10% on safe text | Medium | HIGH | Priority threshold tuning Day 2A; add FP patterns to whitelist; lower threshold if needed |
| R3 | predict.py vs cascade.py confusion | Certain | HIGH | **Benchmark `scan()` only**. Document divergence. Do NOT mix pipelines |
| R4 | Competitor model downloads fail/slow | Medium | Medium | Download LLM Guard + PG2 models Day 1 as background task; have fallback plan |
| R5 | PINT dataset not freely available | Medium | HIGH | Use deepset/prompt-injections as primary; note PINT unavailability if needed |
| R6 | Dataset format mismatch | Medium | Medium | Day 1 EOD sync validates schema; spot-check 50 random samples |
| R7 | Layer 5 model missing weakens story | Certain | Low | Benchmark TF-IDF pipeline only; document L5 as "roadmap"; honest about what ships |
| R8 | Holdout data leaks into tuning | Medium | HIGH | Strict directory separation (`holdout/` vs `benchmark/`); holdout never used in tuning |
| R9 | Na0S FPR is high on real benign data | Medium | HIGH | Run hard-negatives early Day 2; configurable threshold (CP-3) allows finding better operating point; report ROC curves |
| R10 | PyPI name collision | Low | Medium | Check availability before Day 3; have backup name (`na0s-detector`) |
| R11 | CI timeout on full benchmark | Medium | Low | Add `--max-samples 500` for CI; full benchmark on-demand |
| R12 | Coverage 70% gate breaks CI | Medium | Medium | Incremental: 60% Day 3, 70% post-sprint; exclude benchmark scripts from coverage |

---

## 9. Known Detection Gaps (155 @expectedFailure tests)

These represent documented gaps that will show as misses in the benchmark. They should be **acknowledged transparently**, not hidden:

| Category | Gap Count | Nature |
|----------|-----------|--------|
| D6 Multilingual | 40 | ML model has 0 non-English training samples |
| C1 Compliance Evasion | 21 | Fictional framing, hypothetical scenarios |
| E1 Prompt Extraction | 17 | Indirect extraction, social engineering |
| E2 Reconnaissance | 16 | Capability probing, version discovery |
| P1 Privacy Leakage | 14 | Inference attacks, membership inference |
| FP (False Positives) | 11 | Security docs, educational content |
| D4 Obfuscation | 10 | Combined encoding + structural attacks |
| D8 Context Manipulation | 6 | Document overflow, strategic placement |
| D1 Instruction Override | 5 | Subtle paraphrased overrides |
| D7 Payload Delivery | 5 | Fragmented payloads across messages |
| O1 Harmful Content | 4 | Output manipulation edge cases |
| D3 Structural Boundary | 2 | Subtle boundary markers |
| D5 Unicode Evasion | 1 | Edge case |
| A Adversarial ML | **0 tests exist** | GCG, AutoDAN, PAIR -- entire category untested |
| T Agent/Tool Abuse | **0 tests exist** | Tool-call injection -- entire category untested |

**Strategy**: Present benchmark results alongside a "detection coverage matrix" showing where Na0S excels (D4, D5, evasion resistance, unique defenses) vs known gaps (D6, A). This turns gaps into a transparent roadmap rather than a weakness.

---

## Appendix A: Success Metrics

| Metric | Minimum Viable | Stretch Target |
|--------|---------------|----------------|
| F1 on primary dataset | Documented (any honest number) | >= 85% |
| FPR on safe holdout | Documented | < 5% |
| CLI functional | `na0s scan` works | + `na0s bench` |
| JSON output | `.to_json()` works | + CSV format |
| PyPI installable | TestPyPI | Production PyPI |
| Coverage | >= 60% | >= 70% |
| Regression tests | 20 malicious + 20 safe | 50 + 50 |
| Datasets evaluated | >= 3 datasets | All downloaded + holdout + adversarial |
| Competitor comparison | Na0S vs 1 competitor | Na0S vs LLM Guard vs PG2 |
| Documentation | BENCHMARK_RESULTS.md | + updated README |
| Evasion resistance tested | Documented | Per-evasion-type breakdown per tool |

## Appendix B: Key Hardcoded Values

| Constant | File | Current Value | Notes |
|----------|------|---------------|-------|
| `DECISION_THRESHOLD` | predict.py:89 | 0.55 | Made configurable in CP-3; may change Day 2A |
| `ML_WEIGHT` | predict.py:240 | 0.6 | ML multiplier in composite |
| `OBFUSCATION_WEIGHT_CAP` | predict.py:251 | 0.3 | Max obfuscation contribution |
| `SCAN_TIMEOUT` | predict.py | 60s | Wall-clock timeout per scan |
| `MAX_INPUT_CHARS` | layer0/validation.py | 50,000 (env-configurable) | L0 size gate |
| `PARANOIA_LEVEL` | layer1/paranoia.py | 2 (env: RULES_PARANOIA_LEVEL) | Default PL2 = 56 of 66 rules |
| `_ENTROPY_THRESHOLD` | layer2/obfuscation.py:310 | 4.5 | Shannon entropy cutoff |
| `_KL_THRESHOLD` | layer2/obfuscation.py:311 | 0.8 | KL-divergence cutoff |
| Coverage gate | ci.yml | 50% -> 60% (sprint) | Raise further post-sprint |

## Appendix C: File Creation Checklist

### New Files (WS-A)
- [ ] `scripts/benchmark.py`
- [ ] `scripts/wrappers/__init__.py`
- [ ] `scripts/wrappers/base.py`
- [ ] `scripts/wrappers/llm_guard.py`
- [ ] `scripts/wrappers/prompt_guard.py`
- [ ] `scripts/benchmark_report.py`
- [ ] `tests/test_benchmark_regression.py`
- [ ] `BENCHMARK_RESULTS.md`

### New Files (WS-B)
- [x] `src/na0s/cli.py`
- [x] `tests/test_cli.py`
- [x] `Makefile`
- [ ] `Dockerfile`
- [ ] `requirements-benchmark.txt`
- [ ] `data/benchmark/deepset_pi.jsonl`
- [ ] `data/benchmark/benign_alpaca.jsonl`
- [ ] `data/benchmark/benign_dolly.jsonl`
- [ ] `data/benchmark/adversarial_evasion.jsonl`
- [ ] `data/holdout/safe_holdout.jsonl`
- [ ] `data/holdout/malicious_holdout.jsonl`
- [ ] `data/holdout/README.md`
- [ ] `.github/workflows/publish.yml`
